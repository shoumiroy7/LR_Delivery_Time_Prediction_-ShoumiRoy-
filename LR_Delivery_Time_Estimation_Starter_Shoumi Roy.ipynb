{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uOZe3UO68U-"
   },
   "source": [
    "# Order Delivery Time Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qd9e2-HF6_85",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Objectives\n",
    "The objective of this assignment is to build a regression model that predicts the delivery time for orders placed through Porter. The model will use various features such as the items ordered, the restaurant location, the order protocol, and the availability of delivery partners.\n",
    "\n",
    "The key goals are:\n",
    "- Predict the delivery time for an order based on multiple input features\n",
    "- Improve delivery time predictions to optimiae operational efficiency\n",
    "- Understand the key factors influencing delivery time to enhance the model's accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcC6tJ2p7F2p"
   },
   "source": [
    "## Data Pipeline\n",
    "The data pipeline for this assignment will involve the following steps:\n",
    "1. **Data Loading**\n",
    "2. **Data Preprocessing and Feature Engineering**\n",
    "3. **Exploratory Data Analysis**\n",
    "4. **Model Building**\n",
    "5. **Model Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGOQI_f72jV1"
   },
   "source": [
    "## Data Understanding\n",
    "The dataset contains information on orders placed through Porter, with the following columns:\n",
    "\n",
    "| Field                     | Description                                                                                 |\n",
    "|---------------------------|---------------------------------------------------------------------------------------------|\n",
    "| market_id                 | Integer ID representing the market where the restaurant is located.                         |\n",
    "| created_at                | Timestamp when the order was placed.                                                        |\n",
    "| actual_delivery_time      | Timestamp when the order was delivered.                                                     |\n",
    "| store_primary_category    | Category of the restaurant (e.g., fast food, dine-in).                                      |\n",
    "| order_protocol            | Integer representing how the order was placed (e.g., via Porter, call to restaurant, etc.). |\n",
    "| total_items               | Total number of items in the order.                                                         |\n",
    "| subtotal                  | Final price of the order.                                                                   |\n",
    "| num_distinct_items        | Number of distinct items in the order.                                                      |\n",
    "| min_item_price            | Price of the cheapest item in the order.                                                    |\n",
    "| max_item_price            | Price of the most expensive item in the order.                                              |\n",
    "| total_onshift_dashers     | Number of delivery partners on duty when the order was placed.                              |\n",
    "| total_busy_dashers        | Number of delivery partners already occupied with other orders.                             |\n",
    "| total_outstanding_orders  | Number of orders pending fulfillment at the time of the order.                              |\n",
    "| distance                  | Total distance from the restaurant to the customer.                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QoCQFDzHUWP"
   },
   "source": [
    "## **Importing Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jun9CeAc7QOw"
   },
   "outputs": [],
   "source": [
    "# Import essential libraries for data manipulation and analysis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MueJxkvUIII3"
   },
   "source": [
    "## **1. Loading the data**\n",
    "Load 'porter_data_1.csv' as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJS8ZRJXHTwv"
   },
   "outputs": [],
   "source": [
    "# Importing the file porter_data_1.csv\n",
    "\n",
    "df = pd.read_csv('porter_data_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSRQocOkMSQl"
   },
   "source": [
    "## **2. Data Preprocessing and Feature Engineering** <font color = red>[15 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02uPO8aQfLnn"
   },
   "source": [
    "#### **2.1 Fixing the Datatypes**  <font color = red>[5 marks]</font> <br>\n",
    "The current timestamps are in object format and need conversion to datetime format for easier handling and intended functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b22Kzjew3rdM"
   },
   "source": [
    "##### **2.1.1** <font color = red>[2 marks]</font> <br>\n",
    "Convert date and time fields to appropriate data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoGkz909IXjv"
   },
   "outputs": [],
   "source": [
    "# Convert 'created_at' and 'actual_delivery_time' columns to datetime format\n",
    "\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "df['actual_delivery_time'] = pd.to_datetime(df['actual_delivery_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1EBPjFc4Qca"
   },
   "source": [
    "##### **2.1.2**  <font color = red>[3 marks]</font> <br>\n",
    "Convert categorical fields to appropriate data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PihPSPhQq1nQ"
   },
   "outputs": [],
   "source": [
    "# Convert categorical features to category type\n",
    "df['market_id'] = df['market_id'].astype('category')\n",
    "df['store_primary_category'] = df['store_primary_category'].astype('category')\n",
    "df['order_protocol'] = df['order_protocol'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsEGroRFlX8z"
   },
   "source": [
    "#### **2.2 Feature Engineering** <font color = red>[5 marks]</font> <br>\n",
    "Calculate the time taken to execute the delivery as well as extract the hour and day at which the order was placed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BubGzQyJpHLQ"
   },
   "source": [
    "##### **2.2.1** <font color = red>[2 marks]</font> <br>\n",
    "Calculate the time taken using the features `actual_delivery_time` and `created_at`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBGS4PZJMciZ"
   },
   "outputs": [],
   "source": [
    "# Calculate time taken in minutes\n",
    "df['time_taken'] = (df['actual_delivery_time'] - df['created_at']).dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngUUAf3XOPAP"
   },
   "source": [
    "##### **2.2.2** <font color = red>[3 marks]</font> <br>\n",
    "Extract the hour at which the order was placed and which day of the week it was. Drop the unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwA4O5VtNxQW"
   },
   "outputs": [],
   "source": [
    "# Extract the hour and day of week from the 'created_at' timestamp\n",
    "df['order_hour'] = df['created_at'].dt.hour\n",
    "df['day_of_week'] = df['created_at'].dt.dayofweek\n",
    "\n",
    "# Create a categorical feature 'isWeekend'\n",
    "df['isWeekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgzSO8wyOTbP"
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df = df.drop(['created_at', 'actual_delivery_time'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JJxTsQOFKyl"
   },
   "source": [
    "#### **2.3 Creating training and validation sets** <font color = red>[5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuyPJMpCFyUL"
   },
   "source": [
    "##### **2.3.1** <font color = red>[2 marks]</font> <br>\n",
    " Define target and input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVyKFLXTFKRE"
   },
   "outputs": [],
   "source": [
    "# Define target variable (y) and features (X)\n",
    "X = df.drop('time_taken', axis=1)\n",
    "y = df['time_taken']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e56iVNqdF3G8"
   },
   "source": [
    "##### **2.3.2** <font color = red>[3 marks]</font> <br>\n",
    " Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0t7XtNDEF6Pu"
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQxv96NBAq_y"
   },
   "source": [
    "## **3. Exploratory Data Analysis on Training Data** <font color = red>[20 marks]</font> <br>\n",
    "1. Analyzing the correlation between variables to identify patterns and relationships\n",
    "2. Identifying and addressing outliers to ensure the integrity of the analysis\n",
    "3. Exploring the relationships between variables and examining the distribution of the data for better insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VU1baEcRc1-A"
   },
   "source": [
    "#### **3.1 Feature Distributions** <font color = red> [7 marks]</font> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rj7yFI7VJ_va"
   },
   "outputs": [],
   "source": [
    "# Define numerical and categorical columns for easy EDA and data manipulation\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['category']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWMFLWKpHE-R"
   },
   "source": [
    "##### **3.1.1** <font color = red>[3 marks]</font> <br>\n",
    "Plot distributions for numerical columns in the training set to understand their spread and any skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_M0u5G1YR73_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot distributions for all numerical columns\n",
    "plt.figure(figsize=(15, 10))\n",
    "# Calculate how many rows we need based on number of columns\n",
    "num_cols = len(numerical_cols)\n",
    "rows = (num_cols + 2) // 3  # Calculate rows needed (3 columns per row)\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    plt.subplot(rows, 3, i+1)  # Adjust grid size based on number of columns\n",
    "    sns.histplot(X_train[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MtpapIvc9rC"
   },
   "source": [
    "##### **3.1.2** <font color = red>[2 marks]</font> <br>\n",
    "Check the distribution of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zr8loNgMLdrm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribution of categorical columns\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.countplot(x=X_train[col])\n",
    "    plt.title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-9pcLxzJZWf"
   },
   "source": [
    "##### **3.1.3** <font color = red>[2 mark]</font> <br>\n",
    "Visualise the distribution of the target variable to understand its spread and any skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiWe2Bl9R7yL"
   },
   "outputs": [],
   "source": [
    "# Distribution of time_taken\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(y_train, kde=True)\n",
    "plt.title('Distribution of Delivery Time (minutes)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbxczs61dROZ"
   },
   "source": [
    "#### **3.2 Relationships Between Features** <font color = red>[3 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YH81kNkOOvlx"
   },
   "source": [
    "##### **3.2.1** <font color = red>[3 marks]</font> <br>\n",
    "Scatter plots for important numerical and categorical features to observe how they relate to `time_taken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIBnRHohR799"
   },
   "outputs": [],
   "source": [
    "# Scatter plot to visualise the relationship between time_taken and other features\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Get the number of numerical columns\n",
    "n_cols = len(numerical_cols)\n",
    "\n",
    "# Calculate rows and columns needed for the subplots\n",
    "# Using ceil to round up for any fractional result\n",
    "import math\n",
    "n_rows = math.ceil(n_cols / 3)  # 3 columns per row\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    plt.subplot(n_rows, 3, i+1)  # Dynamically calculate rows based on number of features\n",
    "    sns.scatterplot(x=X_train[col], y=y_train)\n",
    "    plt.title(f'{col} vs Delivery Time')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiWL3cKowfZd"
   },
   "outputs": [],
   "source": [
    "# Show the distribution of time_taken for different hours\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x=X_train['order_hour'], y=y_train)\n",
    "plt.title('Delivery Time by Hour of Day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKg6rBljIJFP"
   },
   "source": [
    "#### **3.3 Correlation Analysis** <font color = red>[5 marks]</font> <br>\n",
    "Check correlations between numerical features to identify which variables are strongly related to `time_taken`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cyk00sbYfnc0"
   },
   "source": [
    "##### **3.3.1** <font color = red>[3 marks]</font> <br>\n",
    "Plot a heatmap to display correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxrdHdvKR7vy"
   },
   "outputs": [],
   "source": [
    "# Plot the heatmap of the correlation matrix\n",
    "corr_matrix = X_train[numerical_cols].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yuD3RIwffZE"
   },
   "source": [
    "##### **3.3.2** <font color = red>[2 marks]</font> <br>\n",
    "Drop the columns with weak correlations with the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDZN586gH8R_"
   },
   "outputs": [],
   "source": [
    "# Drop 3-5 weakly correlated columns from training dataset\n",
    "X_train = X_train.drop(['min_item_price', 'max_item_price', 'num_distinct_items'], axis=1)\n",
    "X_test = X_test.drop(['min_item_price', 'max_item_price', 'num_distinct_items'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mZv2rz6lxvc"
   },
   "source": [
    "#### **3.4 Handling the Outliers** <font color = red>[5 marks]</font> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdyAT-OhyH3z"
   },
   "source": [
    "##### **3.4.1** <font color = red>[2 marks]</font> <br>\n",
    "Visualise potential outliers for the target variable and other numerical features using boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ow3Mowo4R71T"
   },
   "outputs": [],
   "source": [
    "# Boxplot for time_taken\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(y=y_train)\n",
    "plt.title('Boxplot of Delivery Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZCaGBKv_stm"
   },
   "source": [
    "##### **3.4.2** <font color = red>[3 marks]</font> <br>\n",
    "Handle outliers present in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwQ1A_wZ_X_K"
   },
   "outputs": [],
   "source": [
    "# Handle outliers\n",
    "def remove_outliers(df, columns):\n",
    "    for col in columns:\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    return df\n",
    "\n",
    "# Apply to training data\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "train_data = remove_outliers(train_data, numerical_cols)\n",
    "X_train = train_data.drop('time_taken', axis=1)\n",
    "y_train = train_data['time_taken']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0Cd2J-LGWaF"
   },
   "source": [
    "## **4. Exploratory Data Analysis on Validation Data** <font color = red>[optional]</font> <br>\n",
    "Optionally, perform EDA on test data to see if the distribution match with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sN6bG_hTbUE"
   },
   "outputs": [],
   "source": [
    "# Define numerical and categorical columns for easy EDA and data manipulation\n",
    "numerical_cols_test = X_test.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols_test = X_test.select_dtypes(include=['category']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Zq16lr0Q9IG"
   },
   "source": [
    "#### **4.1 Feature Distributions**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuoIVgXlQC9y"
   },
   "source": [
    "##### **4.1.1**\n",
    "Plot distributions for numerical columns in the validation set to understand their spread and any skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKgSvKvzG8fv"
   },
   "outputs": [],
   "source": [
    "# Plot distributions for all numerical columns\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numerical_cols_test):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.histplot(X_test[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrywBQGWQC9z"
   },
   "source": [
    "##### **4.1.2**\n",
    "Check the distribution of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0CIcl2tHBwp"
   },
   "outputs": [],
   "source": [
    "# Distribution of categorical columns\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, col in enumerate(categorical_cols_test):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.countplot(x=X_test[col])\n",
    "    plt.title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_j74bnlQC9z"
   },
   "source": [
    "##### **4.1.3**\n",
    "Visualise the distribution of the target variable to understand its spread and any skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dGfR8MHGtqm"
   },
   "outputs": [],
   "source": [
    "# Distribution of time_taken\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(y_test, kde=True)\n",
    "plt.title('Distribution of Delivery Time (minutes)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ki2FI7fsHDgK"
   },
   "source": [
    "#### **4.2 Relationships Between Features**\n",
    "Scatter plots for numerical features to observe how they relate to each other, especially to `time_taken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lzNPoK4HFnZ"
   },
   "outputs": [],
   "source": [
    "# Scatter plot to visualise the relationship between time_taken and other features\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numerical_cols_test):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.scatterplot(x=X_test[col], y=y_test)\n",
    "    plt.title(f'{col} vs Delivery Time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8VoM0XfXWko"
   },
   "source": [
    "#### **4.3** Drop the columns with weak correlations with the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BnM8w2lXWkp"
   },
   "outputs": [],
   "source": [
    "# Drop the weakly correlated columns from training dataset\n",
    "X_test = X_test.drop(['min_item_price', 'max_item_price', 'num_distinct_items'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReNN4PyM8enl"
   },
   "source": [
    "## **5. Model Building** <font color = red>[15 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2l2XfNF6nc8L"
   },
   "source": [
    "#### **Import Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__fmfT6vQWpd"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCLIKw5pQiA7"
   },
   "source": [
    "#### **5.1 Feature Scaling** <font color = red>[3 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "newEgSyyQiHK"
   },
   "outputs": [],
   "source": [
    "# Apply scaling to the numerical columns\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXcV5Z_E8tLL"
   },
   "source": [
    "Note that linear regression is agnostic to feature scaling. However, with feature scaling, we get the coefficients to be somewhat on the same scale so that it becomes easier to compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bxip-t3Y1MB"
   },
   "source": [
    "#### **5.2 Build a linear regression model** <font color = red>[5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7jZciTFtric"
   },
   "source": [
    "You can choose from the libraries *statsmodels* and *scikit-learn* to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMRpgx_iQYM4"
   },
   "outputs": [],
   "source": [
    "# Create/Initialise the model\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbJVZpMiW8b2"
   },
   "outputs": [],
   "source": [
    "# Train the model using the training data\n",
    "lr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCQcJtDbW_dG"
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Udw5kE1fXBsR"
   },
   "outputs": [],
   "source": [
    "# Find results for evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3-HovybcZKR"
   },
   "source": [
    "Note that we have 12 (depending on how you select features) training features. However, not all of them would be useful. Let's say we want to take the most relevant 8 features.\n",
    "\n",
    "We will use Recursive Feature Elimination (RFE) here.\n",
    "\n",
    "For this, you can look at the coefficients / p-values of features from the model summary and perform feature elimination, or you can use the RFE module provided with *scikit-learn*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU8OLQ4bnwdr"
   },
   "source": [
    "#### **5.3 Build the model and fit RFE to select the most important features** <font color = red>[7 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4FZMiX11RyI"
   },
   "source": [
    "For RFE, we will start with all features and use\n",
    "the RFE method to recursively reduce the number of features one-by-one.\n",
    "\n",
    "After analysing the results of these iterations, we select the one that has a good balance between performance and number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ub1HgSwl1eiC"
   },
   "outputs": [],
   "source": [
    "# Loop through the number of features and test the model\n",
    "r2_scores = []\n",
    "for n_features in range(1, len(X_train_scaled.columns)+1):\n",
    "    rfe = RFE(estimator=LinearRegression(), n_features_to_select=n_features)\n",
    "    rfe.fit(X_train_scaled, y_train)\n",
    "    y_pred_rfe = rfe.predict(X_test_scaled)\n",
    "    r2_scores.append(r2_score(y_test, y_pred_rfe))\n",
    "\n",
    "plt.plot(range(1, len(X_train_scaled.columns)+1), r2_scores)\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('R-squared Score')\n",
    "plt.title('R-squared vs Number of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7p-CAQn3wQE"
   },
   "outputs": [],
   "source": [
    "# Build the final model with selected number of features\n",
    "optimal_features = 8  # Based on the plot above\n",
    "rfe = RFE(estimator=LinearRegression(), n_features_to_select=optimal_features)\n",
    "rfe.fit(X_train_scaled, y_train)\n",
    "selected_features = X_train_scaled.columns[rfe.support_]\n",
    "\n",
    "final_model = LinearRegression()\n",
    "final_model.fit(X_train_scaled[selected_features], y_train)\n",
    "y_pred_final = final_model.predict(X_test_scaled[selected_features])\n",
    "\n",
    "final_mse = mean_squared_error(y_test, y_pred_final)\n",
    "final_r2 = r2_score(y_test, y_pred_final)\n",
    "\n",
    "print(f\"Final Model - Mean Squared Error: {final_mse}\")\n",
    "print(f\"Final Model - R-squared: {final_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0l_mLL_4OOl"
   },
   "source": [
    "## **6. Results and Inference** <font color = red>[5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsPGaacJ71mt"
   },
   "source": [
    "#### **6.1 Perform Residual Analysis** <font color = red>[3 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lbj7O8rf7SZS"
   },
   "outputs": [],
   "source": [
    "# Perform residual analysis using plots like residuals vs predicted values, Q-Q plot and residual histogram\n",
    "residuals = y_test - y_pred_final\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.scatterplot(x=y_pred_final, y=residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted Values')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sm.qqplot(residuals, line='s')\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title('Distribution of Residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aq4g9xPsu4T5"
   },
   "source": [
    "[Your inferences here:]\n",
    "\n",
    "The residual plots show that:\n",
    "\n",
    "Residuals vs Predicted: The residuals are randomly scattered around zero, indicating homoscedasticity\n",
    "\n",
    "Q-Q Plot: The points mostly follow the straight line, suggesting normal distribution of residuals\n",
    "\n",
    "Histogram: The residuals are approximately normally distributed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2-CiCId7_y9"
   },
   "source": [
    "#### **6.2 Perform Coefficient Analysis** <font color = red>[2 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2koFJovu-cH"
   },
   "source": [
    "Perform coefficient analysis to find how changes in features affect the target.\n",
    "Also, the features were scaled, so interpret the scaled and unscaled coefficients to understand the impact of feature changes on delivery time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sr8EWhg_9QnI"
   },
   "outputs": [],
   "source": [
    "# Compare the scaled vs unscaled features used in the final model\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Scaled Coefficient': final_model.coef_,\n",
    "    'Unscaled Coefficient': final_model.coef_ / scaler.scale_[rfe.support_]\n",
    "})\n",
    "print(coefficients.sort_values('Scaled Coefficient', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ5VcQ2G-SOb"
   },
   "source": [
    "Additionally, we can analyse the effect of a unit change in a feature. In other words, because we have scaled the features, a unit change in the features will not translate directly to the model. Use scaled and unscaled coefficients to find how will a unit change in a feature affect the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMHN7r-x-Lp5"
   },
   "outputs": [],
   "source": [
    "# Analyze the effect of a unit change in a feature, say 'total_items'\n",
    "total_items_index = list(selected_features).index('total_items')\n",
    "unit_change_effect = coefficients.iloc[total_items_index]['Unscaled Coefficient']\n",
    "print(f\"A one unit increase in total_items changes delivery time by {unit_change_effect:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFWJ2s9I_Yeo"
   },
   "source": [
    "Note:\n",
    "The coefficients on the original scale might differ greatly in magnitude from the scaled coefficients, but they both describe the same relationships between variables.\n",
    "\n",
    "Interpretation is key: Focus on the direction and magnitude of the coefficients on the original scale to understand the impact of each variable on the response variable in the original units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClCit1tvKIyE"
   },
   "source": [
    "Include conclusions in your report document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mn-wDgoeSiHP"
   },
   "source": [
    "## Subjective Questions <font color = red>[20 marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions only in the notebook. Include the visualisations/methodologies/insights/outcomes from all the above steps in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVJSi-Q0Cw_r"
   },
   "source": [
    "#### Subjective Questions based on Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_jiT95xTA6q"
   },
   "source": [
    "##### **Question 1.** <font color = red>[2 marks]</font> <br>\n",
    "\n",
    "Are there any categorical variables in the data? From your analysis of the categorical variables from the dataset, what could you infer about their effect on the dependent variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvFQvBy3VM9A"
   },
   "source": [
    "**Answer:**\n",
    ">Yes, there are categorical variables in the data: market_id, store_primary_category, and order_protocol. From the analysis:\n",
    "1.market_id shows different delivery time patterns across different markets\n",
    "2.store_primary_category indicates that certain restaurant types may have faster/slower preparation times\n",
    "3.order_protocol suggests that different ordering methods may affect delivery efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqPxxtWEY3_W"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDSRymTJTHCW"
   },
   "source": [
    "##### **Question 2.** <font color = red>[1 marks]</font> <br>\n",
    "What does `test_size = 0.2` refer to during splitting the data into training and test sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRBCcZvoVx-r"
   },
   "source": [
    "**Answer:**\n",
    ">test_size=0.2 means that 20% of the data is reserved for testing the model, while 80% is used for training. This is a common split ratio that provides enough data for both training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_afbTV8Y5-F"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEVX57VbTJBP"
   },
   "source": [
    "##### **Question 3.** <font color = red>[1 marks]</font> <br>\n",
    "Looking at the heatmap, which one has the highest correlation with the target variable?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewPqz4yLWBzR"
   },
   "source": [
    "**Answer:**\n",
    ">Based on the correlation analysis, 'distance' has the highest correlation with the target variable (delivery time). This makes intuitive sense as longer distances typically require more delivery time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLy_-8F5Y69c"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lg-6E-N-TKyS"
   },
   "source": [
    "##### **Question 4.** <font color = red>[2 marks]</font> <br>\n",
    "What was your approach to detect the outliers? How did you address them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPUDtsRGWLZl"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    ">Outliers were detected using boxplots and the IQR (Interquartile Range) method. Any data points falling below Q1-1.5*IQR or above Q3+1.5*IQR were considered outliers. These outliers were removed from the training data to prevent them from skewing the model results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVyJFcT2Y7U8"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dvh9CLFnTMhO"
   },
   "source": [
    "##### **Question 5.** <font color = red>[2 marks]</font> <br>\n",
    "Based on the final model, which are the top 3 features significantly affecting the delivery time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-DDpZcCWUun"
   },
   "source": [
    "**Answer:**\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVCrLjhTY74h"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBLH_lA5C4jy"
   },
   "source": [
    "#### General Subjective Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MJGDVyiTOyr"
   },
   "source": [
    "##### **Question 6.** <font color = red>[3 marks]</font> <br>\n",
    "Explain the linear regression algorithm in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZc1QX8RW_Pa"
   },
   "source": [
    "**Answer:**\n",
    ">Linear regression is a supervised learning algorithm that models the linear relationship between independent variables (features) and a dependent variable (target). It works by finding the best-fit line that minimizes the sum of squared residuals (differences between predicted and actual values). The equation is y = β₀ + β₁x₁ + ... + βₙxₙ, where β₀ is the intercept and β₁-βₙ are coefficients for each feature. The algorithm uses ordinary least squares (OLS) to estimate these coefficients by minimizing the cost function (mean squared error).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0MCb30NY8UE"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db_7gqf8TQTk"
   },
   "source": [
    "##### **Question 7.** <font color = red>[2 marks]</font> <br>\n",
    "Explain the difference between simple linear regression and multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1jsR8htXD8j"
   },
   "source": [
    "**Answer:**\n",
    ">Simple linear regression uses only one independent variable to predict the dependent variable (y = β₀ + β₁x). Multiple linear regression uses two or more independent variables (y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ). While simple regression shows the relationship between two variables, multiple regression can account for more complex relationships and interactions between multiple predictors and the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnSGZEltY8ss"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DT6ivEEnTSEs"
   },
   "source": [
    "##### **Question 8.** <font color = red>[2 marks]</font> <br>\n",
    "What is the role of the cost function in linear regression, and how is it minimized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2PaCL-FXSSn"
   },
   "source": [
    "**Answer:**\n",
    ">\n",
    "The cost function (typically mean squared error) measures how far the model's predictions are from the actual values. It quantifies the error of the model. The algorithm minimizes this cost function using optimization techniques like gradient descent or the normal equation (for OLS). In gradient descent, the algorithm iteratively adjusts the coefficients in the direction that reduces the cost function until it reaches the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIKB_W0FY9QM"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZIb5hbMCCVY"
   },
   "source": [
    "##### **Question 9.** <font color = red>[2 marks]</font> <br>\n",
    "Explain the difference between overfitting and underfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8kn4c-7CEjP"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    ">Overfitting occurs when a model learns the training data too well, including noise and outliers, resulting in poor performance on new data. It has high variance. Underfitting occurs when a model is too simple to capture the underlying patterns in the data, performing poorly on both training and test data. It has high bias. The goal is to find the right balance that generalizes well to unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PWIs-suCMEr"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os7JPKHwArn7"
   },
   "source": [
    "##### **Question 10.** <font color = red>[3 marks]</font> <br>\n",
    "How do residual plots help in diagnosing a linear regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqxU8GSkAubl"
   },
   "source": [
    "**Answer:**\n",
    ">Residual plots help diagnose several aspects of a linear regression model:\n",
    "1.Checking linearity: Randomly scattered residuals indicate a linear relationship\n",
    "2.Identifying heteroscedasticity: Funnel-shaped patterns suggest non-constant variance\n",
    "3.Detecting outliers: Points far from zero may be influential outliers\n",
    "4.Assessing normality: Q-Q plots show if residuals follow normal distribution\n",
    "These diagnostics help verify model assumptions and guide improvements.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MueJxkvUIII3",
    "02uPO8aQfLnn",
    "b22Kzjew3rdM",
    "u1EBPjFc4Qca",
    "-JJxTsQOFKyl",
    "v0Cd2J-LGWaF",
    "fCLIKw5pQiA7",
    "2bxip-t3Y1MB",
    "mn-wDgoeSiHP"
   ],
   "provenance": [
    {
     "file_id": "1qHefVpjLoVZcdohzmYySNZGiPR4wQOFp",
     "timestamp": 1737728120597
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
